{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8x7NMv2HnLL"
   },
   "source": [
    "# HW 2\n",
    "\n",
    "For this homework you will be using pytorch and torchvision library for neural networks and datasets. You can install them with pip install torch torchvision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CN-0glSHOTJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_xRwMS2HpzK"
   },
   "source": [
    "# 1 Question 1 Principal Component Analysis\n",
    "\n",
    "This problem will guide you through the principal component analysis. You will be using a classical dataset, the MNIST hand written digit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-q4YGtwHqg8"
   },
   "outputs": [],
   "source": [
    " # Load the MNIST dataset\n",
    "mnist = MNIST('.', download=True)\n",
    "data = mnist.train_data.numpy()\n",
    "labels = mnist.train_labels.numpy()\n",
    "print('shapes:', data.shape, labels.shape)\n",
    "plt.imshow(data[0])\n",
    "print('label:', labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QoT_1_fH57B"
   },
   "source": [
    "### 1.1 Step 1.1 Familiarize yourself with the data\n",
    "\n",
    "For this task, you will be using the torchvision package that provides the MNIST dataset. For each digit class(0-9), plot 1 image from the class and store those 10 images for each digit class in the array digit_images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n149sRzcHvH5"
   },
   "outputs": [],
   "source": [
    "digit_images = np.zeros([10, 28, 28])\n",
    "for i in range(10):\n",
    "  for x,y in zip(data, labels):\n",
    "    if y == i:\n",
    "      digit_images[i] = x\n",
    "      break\n",
    "  plt.imshow(digit_images[i])\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uia56QmfIlJT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySHV4MkxIy3t"
   },
   "source": [
    "## 2 Question 2. PCA\n",
    "\n",
    "This section deals with performing PCA on the MNIST dataset. We will explore 2 different methods of performing PCA. First using SVD, second using Eigen Decomposition. Please refer to the lecture slides, in particular slides 30-32 from Lec 3 which can be found [here](https://ucsd-cse-152.github.io/FA20/slides/Lec3.pdf).\n",
    "\n",
    "### 2.1 Question 2.1 Centering the data [5 points]\n",
    "\n",
    "For each image, flatten it to a 1-D vector. To perform PCA on the dataset, we first move the data points so they have 0 mean on each dimension. Store the centered data in variable **data_centered** and the mean of each dimension in variable **data_mean**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3FOwDM6I5p5"
   },
   "outputs": [],
   "source": [
    "flattened_d= np.array([data_f.flatten() for data_f in data]) #flatten matrix\n",
    "data_mean = np.mean(flattened_d, axis = 0) \n",
    "data_centered = flattened_d - data_mean\n",
    "\n",
    "print(flattened_d.shape)\n",
    "\n",
    "# hint: You need np.mean() and x.reshape() functions (x is a matrix). \n",
    "### END OF CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmPDE2ZyJ96q"
   },
   "source": [
    "### 2.2 Compute the Singular Value Decomposition (SVD) for the given data [5 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdhwqrXIJJa6"
   },
   "outputs": [],
   "source": [
    "U,S,V = None, None, None\n",
    "### YOUR CODE HERE\n",
    "\n",
    "U,S,V = np.linalg.svd(data_centered, full_matrices = False) #S is shape (60000,28)\n",
    "print(U,S,V)\n",
    "### END OF CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABZjCtNVbnAk"
   },
   "source": [
    "### Question 2.3 Project data onto the first 2 principal components [5 points]\n",
    "\n",
    "Now you need to project the centered data on the 2D space formed by the eigenvectors corresponding to the 2 largest eigenvalues. Create a 2D scatter plot where you need to assign a unique color to each digit class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrccMkKOKUnX"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "# this is the library for visualization\n",
    "# you can use sns.scatterplot() function to visualize a scattered plot of the results\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "S = np.square(S)\n",
    "#take either 2 first columns of U or 2 first rows of V\n",
    "eigv = [V[0], V[1]]\n",
    "projected = np.asmatrix(eigv) @ data_centered.T\n",
    "\n",
    "\n",
    "\n",
    "### END OF CODE\n",
    "\n",
    "df = pd.DataFrame(data = projected, columns=['comp1', 'comp2'])\n",
    "df['labels'] = pd.Series(labels)\n",
    "sns.scatterplot(\n",
    "    x=\"comp1\", y=\"comp2\",\n",
    "    hue=\"labels\",\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data=df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivw9YIDtd06F"
   },
   "source": [
    "### Question 2.4 Unproject data back to high dimensions [5 points]\n",
    "For this question, you need to project the 10 images you plotted in **1.2.3** on the first 2 principal components, and then unproject the \"compressed\" 2-D representations back to the original space. Plot the \"compressed\" digit (the reconstructed digit). Do they look similar to the original images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvfQMPzpbe0A"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "eigvectors = [V[i] for i in range(20)]\n",
    "#tranform the test images\n",
    "reduced_test_image = (current_test_image - data_mean) @ eigvectors \n",
    "#transform them back \n",
    "reconstructed = (reduced_test_image @ eigenvectors.T) + data_mean\n",
    "### END OF CODE\n",
    "\n",
    "digit_images = np.zeros([10, 28, 28])\n",
    "for i in range(10):\n",
    "  for x,y in zip(reconstructed, labels):\n",
    "    x = x.reshape(28,28)\n",
    "    if y == i:\n",
    "      digit_images[i] = x\n",
    "      break\n",
    "  plt.imshow(digit_images[i])\n",
    "  plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qj4Gu5NeHYf"
   },
   "source": [
    "### Question 2.5 Choose a better low dimension space. [5pt]\n",
    "Do the previous problem with more dimensions (e.g. 3, 5, 10, 20, 50, 100). You only need to show results for one of them. Answer the following questions. How many dimensions are required to represent the digits reasonably well? Briefly explain what criteria you used to select the number of dimensions and how it relates to the SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOHuk1VBe4dG"
   },
   "source": [
    "### Question 2.6 PCA Using Eigen Decomposition\n",
    "\n",
    "Now we will perform PCA using method 2: Eigen Decomposition.\n",
    "\n",
    "For this method, we need to compute the covariance matrix of the data first. Prof. Su has added three slides at the end of Lec 4 (https://ucsd-cse-152.github.io/FA20/slides/Lec3.pdf) from P30-P32. Please read the slides before you proceed. \n",
    "\n",
    "### Question 2.6.1 Compute the covariance matrix of the data [5 points]\n",
    "You need to store the covariance matrix of the data in variable data_covmat. You may not use numpy.cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Y71pba6hkk7"
   },
   "outputs": [],
   "source": [
    "data_covmat = None\n",
    "### YOUR CODE HERE\n",
    "data_covmat =  1/len(data_centered)*data_centered.T @ data_centered\n",
    "### END OF CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q56gE7MiiJBd"
   },
   "source": [
    "### Question 2.6.2 Compute the eigenvalues and eigen vectors of the covariance matrix [5pt]\n",
    "You need to store the eigenvalues of the covariance matrix in variable `covmat_eig`, sorted in descending order and the corresponding eigenvectors in `covmat_eigvec`. Then you need to plot the eigenvalues with `plt.plot`. You can use any numpy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPdGjB9KrPSp"
   },
   "outputs": [],
   "source": [
    "covmat_eigvec, covmat_eig = None, None\n",
    "### YOUR CODE HERE\n",
    "# hint: you may use np.linalg.eig\n",
    "covmat_eig, covmat_eigvec = np.linalg.eig(data_covmat)\n",
    "idx = np.argsort(covmat_eig)[::-1]\n",
    "evecs = covmat_eigvec[:,idx]\n",
    "eig = covmat_eig[idx]\n",
    "plt.plot(eig)\n",
    "plt.xlabel(\"dimension\")\n",
    "plt.ylabel(\"eigenvalues\")\n",
    "### END OF CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tg7EZxsDrwg0"
   },
   "source": [
    "### Question 2.6.3 Project data onto the first 2 principal components [5 points]\n",
    "\n",
    "Now you need to project the centered data on the 2D space formed by the eigenvectors corresponding to the 2 largest eigenvalues. Create a 2D scatter plot where you need to assign a unique color to each digit class. Feel free to use any plotting libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPjBm2SvsDN4"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "### YOUR CODE HERE\n",
    "projected = None\n",
    "import matplotlib.cm as cm\n",
    "data_PCA = covmat_eigvec[:,:2].T @ data_centered.T\n",
    "colors = cm.rainbow(np.linspace(0, 1, 10))\n",
    "for i in range(10):\n",
    "    l = np.where(labels == i)\n",
    "    plt.scatter(data_PCA[0][l],data_PCA[1][l], s = 0.1, c = colors[i]) \n",
    "### END OF CODE\n",
    "df = pd.DataFrame(data = projected, columns=['comp1', 'comp2'])\n",
    "df['labels'] = pd.Series(labels)\n",
    "sns.scatterplot(\n",
    "    x=\"comp1\", y=\"comp2\",\n",
    "    hue=\"labels\",\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data=df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTcCBTJssgxy"
   },
   "source": [
    "### Question 2.6.4 Compare the 2 approaches\n",
    "\n",
    "What if any is the difference/similariy in computing PCA between the 2 approaches ? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bu6KbNUm1MvW"
   },
   "source": [
    "## Question 3: Neural Networks for Regression\n",
    "\n",
    "As an introduction to the idea of performing regression using Neural Networks, we will first try to build a basic MultiLayer Perceptron (MLP) for fitting a line to random data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6g4PaT5dBSI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "\n",
    "torch.manual_seed(1)    # reproducible\n",
    "\n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)\n",
    "y = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)\n",
    "\n",
    "# torch can only train on Variable, so convert them to Variable\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "# view data\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n",
    "plt.title('Regression Analysis')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAqH_GAQ2iyP"
   },
   "source": [
    "### 3.1 Fitting a basic model\n",
    "\n",
    "Let's try to fit a basic 2 layer network for this given data. We will use one hidden layer with 10 neurons/units and the output layer has one neuron to predict a single value. For this given data, given the x value our goal is to predict a y value which most accurately describes the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxiPbmIO27EW"
   },
   "source": [
    "The basic pytorch work flow involves 4 steps:\n",
    "- Implement the model and initialize it\n",
    "- Decide on the right optimizer\n",
    "- Choose an appropriate loss function for your learning task\n",
    "- train the model over the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2dF1DKAgJHZ"
   },
   "outputs": [],
   "source": [
    "# this is one way to define a network\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "\n",
    "net = Net(n_feature=1, n_hidden=10, n_output=1)     # define the network\n",
    "print(net)  # net architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdxLvWEQ4bwi"
   },
   "source": [
    "For this simple model we will be using the MSE Loss which is the most common and simple loss function used for regression tasks. The MSE loss is the l2 norm of the difference. \n",
    "The MSE loss is given by:\n",
    "$$\n",
    "L(x,y) = \\frac{1}{N}\\Sigma_{i=1}^{i=n} (x - y)^2\n",
    "$$\n",
    "where $N$ is the number of data points\n",
    "\n",
    "For our optimizer we will use Stochastic Gradient Descent (SGD) which is sufficient for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkRGal3b3W88"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xg2cvCC897L"
   },
   "source": [
    "We now train the network on the data for 200 epochs. The training process involves first forward propogating the input through the network after which we calculate the loss value from the current prediction. We then backpropogate the gradient of the loss all the way back through the network. This is done by the `loss.backward()` statement. Once the gradients have been computed we update the weights which in pytorch can be easily done by calling the `optimizer.step()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VioH4xoouAMY"
   },
   "outputs": [],
   "source": [
    "# train the network\n",
    "for t in range(200):\n",
    "    prediction = net(x)     # input x and predict based on x\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wifjt0B79-xh"
   },
   "source": [
    "Let's compare how our model performs by plotting the line fit by our basic MLP on top of the data that we generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tzbfj-wp9vyT"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,7))    \n",
    "ax.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\", label='ground truth')\n",
    "ax.plot(x.data.numpy(), prediction.data.numpy(), 'g-', lw=3, label='prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRyquY6x-vAn"
   },
   "source": [
    "### Q3.2 Fit a model for the sin function\n",
    "\n",
    "We tried to fit a model to a basic randomly generated dataset. Let's now try to implement a MLP that can fit the sin function. We start once again by generating a dataset for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAlnvr6ext7g"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.unsqueeze(torch.linspace(-10, 10, 1000), dim=1)  # x data (tensor), shape=(100, 1)\n",
    "y = torch.sin(x) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)\n",
    "\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n",
    "plt.title('Regression Analysis')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFVTXz6E_Bjm"
   },
   "source": [
    "### Q 3.2 Implement the MLP [5 points]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Fill in the template code below to configure your model. Implement the model description as well as the forward pass for the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsiGtyedyX8D"
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "      super(Net, self).__init__()\n",
    "      \n",
    "      self.fc1 = torch.nn.Linear(1, 200)\n",
    "      # ... continue to construct the network\n",
    "      ### YOUR CODE HERE\n",
    "      self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        # activation\n",
    "      self.relu = nn.ReLU()\n",
    "        # output layer\n",
    "      self.layer2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "      ### END OF CODE\n",
    "\n",
    "    def forward(self, x):\n",
    "     torch.flatten(x, 1)\n",
    "     x = self.layer1(x)\n",
    "     x = self.relu(x)\n",
    "     x = self.layer2(x)\n",
    "     return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIGPI_eU_0Zo"
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 200\n",
    "\n",
    "torch_dataset = Data.TensorDataset(x, y)\n",
    "\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, num_workers=2,)\n",
    "\n",
    "\n",
    "# start training\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        \n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y)\n",
    "\n",
    "        prediction = net(b_x)     # input x and predict based on x\n",
    "\n",
    "        loss = loss_func(prediction, b_y)\n",
    "\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJltr2A3y3KX"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "# ax.set_title('Regression Analysis - model 3, Batches', fontsize=35)\n",
    "ax.set_xlabel('x', fontsize=24)\n",
    "ax.set_ylabel('sin(x)', fontsize=24)\n",
    "ax.set_xlim(-11.0, 13.0)\n",
    "ax.set_ylim(-1.1, 1.2)\n",
    "ax.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\", alpha=0.2)\n",
    "prediction = net(x)     # input x and predict based on x\n",
    "ax.scatter(x.data.numpy(), prediction.data.numpy(), color='green', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rteDuMr3Bwy1"
   },
   "source": [
    "## Q4. Corner detection using CNNs\n",
    "\n",
    "In this section we will implement a Convolution Neural Network(CNN) for detecting corners in images. In particular given an image with a corner in it, the CNN will output the coordinates of the corner in the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzvBpSMXC8fu"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60AMhNgyUzTg"
   },
   "outputs": [],
   "source": [
    "### Only run this cell if you're mounting google drive and using it to acccess \n",
    "### the data. If using please change the path accordingly\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/.....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80_j9p5JC_P-"
   },
   "outputs": [],
   "source": [
    "data, labels = pickle.load(open('./corner_dataset_100x100.pick', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHR2tjqORzXu"
   },
   "outputs": [],
   "source": [
    "tensor_x = torch.Tensor(data) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(labels)\n",
    "\n",
    "dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "train_split, test_split = int(0.75 * len(dataset)), int(0.25 * len(dataset) + 1) \n",
    "print(train_split, test_split, len(dataset))\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [train_split, test_split])\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)\n",
    "# dataloader = DataLoader(dataset, batch_size =128, shuffle = True) # create your dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJse9OPH97cR"
   },
   "source": [
    "### Q 4.1 Implement the CNN [10 points]\n",
    "Complete the code below to implement your model. You will have design your own CNN that can detect corners in images. You can find the list of layers available as part of pytorch [here](https://pytorch.org/docs/stable/nn.html). You will use a combination of the following layers:\n",
    "- Convolution layer - this layer performs convolution on the images. [doc](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
    "- MaxPool Layer - maxpool layers are used to reduce the dimensions of feature maps while maintaining translation invariance. [DOC](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)\n",
    "- Flatten - Since the output of the convolution layers are image like feature maps you will need a layer which can convert or flatten the image into a single array. This layer does that. [DOC](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten)\n",
    "- Non Linearities - You need a non-linear activation function after each layer. You have several choices for these such as sigmoid, relu etc. [DOC](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKxWx5yRU903"
   },
   "outputs": [],
   "source": [
    "# this is one way to define a network\n",
    "class CornerNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CornerNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(1,32, kernel_size=(3,3), stride=1, padding=1)        \n",
    "        # ... continue the construction of the network\n",
    "        ### YOUR CODE HERE\n",
    "        # Hint: You may need torch.nn.Conv2d(), torch.nn.MaxPool2D(), torch.nn.Flatten(), torch.nn.Linear(), torch.nn.Sigmoid(), torch.nn.ReLU()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32 , 64, 5,stride=1, padding=2, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(3136, num_classes)\n",
    "        )\n",
    "        \n",
    "        ### END OF CODE\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.conv1(x)\n",
    "      ### YOUR CODE HERE\n",
    "      # make sure to be consistent with your network definition in __init__() function of this class  \n",
    "      x = self.first(x)\n",
    "      x = self.main(x)\n",
    "      x = x.reshape(x.size(0), -1)\n",
    "      x = self.fc(x)\n",
    "      ### END OF CODE\n",
    "\n",
    "      return x\n",
    "        \n",
    "\n",
    "net = CornerNet((1, 28, 28), 10)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVNaKCytB5on"
   },
   "source": [
    "Feel free to change and explore hyperparameters such as learning rate (lr) and number of epochs. You may use different optimizer or criterions as well to improve your accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ja6A66hIWlRR"
   },
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "optimizer = torch.optim.Adagrad(net.parameters(), lr = lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "cuda_status = torch.cuda.is_available()\n",
    "\n",
    "if cuda_status:\n",
    "  net = net.cuda()\n",
    "  \n",
    "net.train()\n",
    "for epoch in range(80):\n",
    "  training_loss = 0\n",
    "  for i,(x,y) in enumerate(trainloader):\n",
    "    y /= 100\n",
    "    if cuda_status:\n",
    "      y = y.cuda()\n",
    "      x = x.cuda()\n",
    "    x = torch.unsqueeze(x,1)\n",
    "    out = net(x)\n",
    "    loss = criterion(out, y)\n",
    "    training_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  training_loss /= i\n",
    "  print(\"\\tEpoch: {} Loss: {}\".format(epoch, training_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHq6U6loDElL"
   },
   "source": [
    "We will now evaluate our trained network on the test dataset. Please report the final test loss your network achieves on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qojHtLnddve"
   },
   "outputs": [],
   "source": [
    "net.cpu()\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "  test_loss = 0\n",
    "  for i,(x,y) in enumerate(testloader):\n",
    "    y /= 100\n",
    "    x = torch.unsqueeze(x, 1)\n",
    "    coords = net(x)\n",
    "    loss = criterion(y, coords).item()\n",
    "    test_loss += loss\n",
    "  test_loss /= i\n",
    "  print(\"Test loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dC79M-wsDLFR"
   },
   "source": [
    "### Visualize performance\n",
    "\n",
    "Let's try to visualize the performance of the model. We will plot the image first and then overlay the coordinates of the corner on top of the image to see if the location is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvYuwIpSex8g"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  for i,(x,y) in enumerate(testloader):\n",
    "    x = torch.unsqueeze(x, 1)\n",
    "    coords = net(x)\n",
    "\n",
    "    for j in range(10):\n",
    "      fig, ax = plt.subplots()\n",
    "      ax.imshow(x[j][0], cmap=plt.cm.gray)\n",
    "      pts = y[j]\n",
    "      pts = coords[j] * 100\n",
    "      print(pts)\n",
    "      print(y[j])\n",
    "      ax.plot(pts[1], pts[0], color='red', marker='o',\n",
    "              linestyle='None', markersize=6)\n",
    "      plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lrdpvlIgB35"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CSE152_HW_2_edited.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
